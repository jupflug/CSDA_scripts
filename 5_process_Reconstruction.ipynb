{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35f2652-bcc9-4b36-ae38-e1241fa3ff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading dependencies and plotting options\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib.dates import DateFormatter\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c510750-7033-4bf5-a306-36f071802639",
   "metadata": {},
   "source": [
    "## SET USER DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be23e19-c7c9-4165-9f78-62ee4b41ae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOMID = 'DAN'\n",
    "\n",
    "# specify the year the reconstruction is being processed\n",
    "year_value = 2023\n",
    "# provided the DSD data blocks from 3_process_DSD.ipynb, what index does this year correspond to?\n",
    "year_idx = 4\n",
    "\n",
    "meadows_direc = '/Users/jpflug/Documents/Projects/cubesatReanaly/Data/Meadows/'+DOMID+'/self_classified/SCA/'\n",
    "# dsd file for this year\n",
    "dsd_file = meadows_direc+'processed_'+str(year_value)+'/DSD.npy'\n",
    "# time series of snow cover and correpdonding dates\n",
    "dsd_cubeFile = meadows_direc+'processed_'+str(year_value)+'/DSD_stacked.npy'\n",
    "dsd_daydiffFile = meadows_direc+'processed_'+str(year_value)+'/DSD_daydiff.npy'\n",
    "# 4 snow classes (see 4_process_SnowClass.ipynb)\n",
    "class_file = meadows_direc+'classified_quartile.npy'\n",
    "# base directory for the snow pillow data\n",
    "pillow_direc = '/Users/jpflug/Documents/Projects/cubesatReanaly/Data/PillowData/'+DOMID+'/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf37ec-db24-4e23-849e-ed16afdc279b",
   "metadata": {},
   "source": [
    "#### Load data from previous processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890d5e8-63c2-476a-a599-651c09e4228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the date of snow disappearance for the reconstruction year\n",
    "dsd = np.load(dsd_file)[year_idx,:,:]\n",
    "# plot DSD for a sanity check\n",
    "fg,ax = plt.subplots(figsize=(10,10))\n",
    "out = ax.imshow(dsd,interpolation='none')\n",
    "fg.colorbar(out)\n",
    "\n",
    "# load the snow cover map data cube and corresponding dates (see 3_process_DSD)\n",
    "dsdCube = np.load(dsd_cubeFile)\n",
    "daydiff = np.load(dsd_daydiffFile)\n",
    "# load the 4 unique snow classes (see 4_process_SnowClass)\n",
    "snow_classes = np.load(class_file)\n",
    "# plot as a sanity check\n",
    "fg,ax = plt.subplots(figsize=(10,10))\n",
    "ax.imshow(snow_classes)\n",
    "# process the class ID's to loop through later in the script\n",
    "class_numbers = np.unique(snow_classes)\n",
    "class_numbers = class_numbers[~np.isnan(class_numbers)]\n",
    "\n",
    "# load the snow pillow melt factor (MF), snowmelt onset date (MD), and shortwave radiation (dailySW)\n",
    "start_wy = pd.Timestamp(str(year_value-1)+'-10-01 00:00:00')\n",
    "MF = np.load(pillow_direc+str(year_value)+'/MF.npy')\n",
    "MD = np.load(pillow_direc+str(year_value)+'/MD.npy',allow_pickle=True).item()\n",
    "MD = (MD-start_wy).days\n",
    "dailySW = np.load(pillow_direc+str(year_value)+'/meltSeason_SW.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e44e7-c220-49eb-8ad5-0b5cff1f75cf",
   "metadata": {},
   "source": [
    "#### Process the full domain fSCA evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5d3669-b415-497a-9929-780ba1392e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask the stacked snow binary presence/absence map\n",
    "classes_stacked = np.tile(snow_classes,[len(daydiff),1,1])\n",
    "dsdCube[np.isnan(classes_stacked)] = np.nan\n",
    "# classify the total modeling area based on the snow class map\n",
    "total_area = len(snow_classes[~np.isnan(snow_classes)])\n",
    "# calculate the fsca for the full domain\n",
    "fsca = np.nansum(dsdCube,axis=(1,2))/total_area\n",
    "\n",
    "# reduce the period to avoid overweighting on zero snow dates for reconstruction Methods 2 and 3\n",
    "idx_start = 0\n",
    "idx_end = np.where(fsca <= 0.01)\n",
    "if len(idx_end) >= 1:\n",
    "    try:\n",
    "        idx_end = idx_end[0][0]+10\n",
    "    except:\n",
    "        idx_end = idx_end[0][0]+1\n",
    "else:\n",
    "    idx_end = len(fsca)+1\n",
    "daydiff_filt = daydiff[idx_start:idx_end]\n",
    "obs_date_list = [datetime.datetime(year_value-1, 10, 1) + datetime.timedelta(days=d) for d in daydiff_filt.astype(list)]\n",
    "fsca_filt = fsca[idx_start:idx_end]\n",
    "dsdCube_filt = dsdCube[idx_start:idx_end,:,:]\n",
    "dsd_filt = dsd.copy()\n",
    "# filter the dsd map by only grid cells falling within the unique snow classes\n",
    "dsd_filt[np.isnan(snow_classes)] = np.nan\n",
    "\n",
    "# plot the full-domain fsca versus time\n",
    "fg,ax = plt.subplots()\n",
    "ax.scatter(daydiff,fsca)\n",
    "ax.scatter(daydiff[idx_start:idx_end],fsca[idx_start:idx_end],10,'r')\n",
    "ax.plot([daydiff[idx_start],daydiff[idx_start]],[0,1],'--k')\n",
    "# for reference, plot the snow pillow oberved date of snowmelt onset (MD)\n",
    "ax.plot([MD,MD],[0,1],'--k')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5147502c-0785-446a-975e-9e7aa0dae4c5",
   "metadata": {},
   "source": [
    "#### Using reconstruction Method 1, estimate SWE on the initial spring date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebe88e0-27ca-4ac5-8848-f19bb73552ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the unique PS-observed snow disappearance dates\n",
    "dsd_unique = np.unique(dsd)\n",
    "dsd_unique = dsd_unique[~np.isnan(dsd_unique)]\n",
    "\n",
    "# initialize\n",
    "SWE_map_base = np.empty(dsd.shape)\n",
    "SWE_map_base[:] = np.nan\n",
    "# loop through the snow disappearance dates\n",
    "for dsd_sel in dsd_unique:\n",
    "    # check to see if the dsd date (daydiff_filt[0]) is before the modeling date, if so than the SWE disappears on that date\n",
    "    if dsd_sel < daydiff_filt[0]:\n",
    "        SWE_map_base[dsd_filt == dsd_sel] = 0\n",
    "    else:\n",
    "        # determine the cumulative radiation over this period\n",
    "        swsum = dailySW.copy()\n",
    "        swsum[:daydiff_filt[0]] = 0\n",
    "        swsum = np.cumsum(swsum)[int(dsd_sel)]\n",
    "        # calculate the SWE based on the pillow-defind MF\n",
    "        SWE_map_base[dsd_filt == dsd_sel] = (swsum*MF[0])+MF[1]\n",
    "\n",
    "# plot the Method 1 SWE estiamte on the initial date (t = daydiff_filt[0])\n",
    "fg,ax = plt.subplots(figsize=(10,10))\n",
    "out = ax.imshow(SWE_map_base,vmin=0,vmax=4.5,cmap='Blues')\n",
    "fg.colorbar(out)\n",
    "ax.set_facecolor([0.6,0.6,0.6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04551b-611f-4942-b4c7-2a966c4567fc",
   "metadata": {},
   "source": [
    "#### Method 2: Assume uniform melt, but fit the melt rate using an assumption of meadow-scale heterogeneity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b11d126-7e20-42ac-a0dc-24e96e33947c",
   "metadata": {},
   "outputs": [],
   "source": [
    "############## FUNCTIONS -- DO NOT CHANGE ################\n",
    "# lognormal distribution generator given mean SWE and CV\n",
    "def get_SWEdist(D,meanSWE,CV):\n",
    "    zeta = np.sqrt(np.log(1 + (CV**2)))\n",
    "    lam = np.log(meanSWE) - (0.5*(zeta ** 2))\n",
    "    fD = (1/(D*zeta*np.sqrt(2*np.pi)))*np.exp(-0.5*((np.log(D)-lam)/zeta)**2)\n",
    "    fD = fD/np.sum(fD)\n",
    "    return fD\n",
    "# approach from Liston (2004) that calculates fSCA from portion of distribution higher than melt    \n",
    "def listonApproach(D,meanSWE,CV,SW,MD,MF,offset):    \n",
    "    # lognormal distribution generator given mean SWE and CV\n",
    "    fD = get_SWEdist(D,meanSWE,CV)\n",
    "    # calculate the cumulative shortwave radiation\n",
    "    SW[:MD] = 0\n",
    "    SW = np.cumsum(SW)\n",
    "    # initialize and calculate the melt\n",
    "    fsca_out = []\n",
    "    # loop through the time record of shortwave data\n",
    "    for sw_sel in SW:\n",
    "        # calculate melt and shift the distribution uniformly\n",
    "        melt = (sw_sel * MF) + offset\n",
    "        newSWE = D - melt\n",
    "        # calculate fSCA as the portion of the shifted distribution that is above zero\n",
    "        fsca_out.append(np.sum(fD[newSWE > 0]))\n",
    "    fsca_out = np.array(fsca_out)\n",
    "    return fsca_out\n",
    "############ END FUNCTIONS FOR METHOD 2 #################\n",
    "\n",
    "## USER DEFINITIONS ##\n",
    "# possible range of SWE values\n",
    "D = np.linspace(0.0001,7,2000)\n",
    "# range of SWE CoVs\n",
    "C_array = np.linspace(0.05,0.5,20)\n",
    "# range of possible melt factors\n",
    "B_array = np.linspace(MF[0]*0.5,MF[0]*1.5,20)\n",
    "\n",
    "# initialize\n",
    "MSE = []\n",
    "C_saver,B_saver,offset_saver = [],[],[]\n",
    "\n",
    "fg,ax = plt.subplots()\n",
    "# loop through the coefficients of variation\n",
    "for C in C_array:\n",
    "    # calculate the initial distribution based on the CoV and mean SWE\n",
    "    temp_dist = get_SWEdist(D,np.nanmean(SWE_map_base),C)\n",
    "    # allow the initial distribution to be shifted by +- 10%, loop through these shifts\n",
    "    offsets = np.linspace(-0.1*np.nanmean(SWE_map_base),0.1*np.nanmean(SWE_map_base),20)\n",
    "    for offset in offsets:\n",
    "        # loop through the melt factors\n",
    "        for B in B_array:\n",
    "            # append each combination of model parameters into arrays\n",
    "            C_saver.append(C)\n",
    "            B_saver.append(B)\n",
    "            offset_saver.append(offset)\n",
    "            # fSCA output for this combination of model parameters (CVs, offsets, and melt factors)\n",
    "            test_output = listonApproach(D,np.nanmean(SWE_map_base),C,dailySW,daydiff_filt[0],B,offset) \n",
    "            # plot a gray trace for each ensemble member's fSCA evolution\n",
    "            ax.plot(test_output,color=[0.75,0.75,0.75])\n",
    "            # calculate model performance and save -- using mean squared error\n",
    "            MSE.append(np.mean((test_output[daydiff_filt]-fsca_filt)**2))\n",
    "\n",
    "# plot the planet-observed fSCA evolution\n",
    "ax.scatter(daydiff_filt,fsca_filt,20,'k',zorder=100)\n",
    "# find best performing ensemble member and corresponding model parameters\n",
    "best_idx = np.where(MSE == np.nanmin(MSE))[0][0]\n",
    "B_single = B_saver[best_idx]\n",
    "C_single = C_saver[best_idx]\n",
    "offset_single = offset_saver[best_idx]\n",
    "print('best model parameters')\n",
    "print('Melt Factor: 'B_single)\n",
    "print('SWE CoV: ',C_single)\n",
    "print('Initial SWE offset: ',offset_single)\n",
    "\n",
    "# re-generate the fSCA evolution using the best SWE parameters for Method 2, plot for comparison versus PS fSCA observations\n",
    "test_output = listonApproach(D,np.nanmean(SWE_map_base),C_single,dailySW,daydiff_filt[0],B_single,offset_single) \n",
    "ax.plot(test_output,'--k')\n",
    "\n",
    "### save the data, if desired ######\n",
    "# np.save(meadows_direc+'processed_'+str(year_value)+'/Method2_MeltFactor.npy',B_single)\n",
    "# np.save(meadows_direc+'processed_'+str(year_value)+'/Method2_CoV.npy',C_single)\n",
    "# np.save(meadows_direc+'processed_'+str(year_value)+'/Method2_offset.npy',offset_single)\n",
    "#####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df256723-df69-47bf-a958-0f680b8de5aa",
   "metadata": {},
   "source": [
    "#### Method 3: Assume uniform melt, but fit the melt rate using an assumption of SWE heterogeneity in four snow classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f593eb-680b-4c50-b148-620e2f47d595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize\n",
    "B_class,C_class,offset_class = [],[],[]\n",
    "# loop through the specific class numbers, \n",
    "# repeat the approach for Method 2 above, but for the unique classes\n",
    "for class_no in class_numbers:\n",
    "    # initialize, subclass\n",
    "    MSE = []\n",
    "    C_saver,B_saver,offset_saver = [],[],[]\n",
    "    \n",
    "    # mask the stacked snow binary presence/absence map based on the snow class of focus\n",
    "    classes_stacked = np.tile(snow_classes,[len(daydiff),1,1])\n",
    "    dsdCube_classes = dsdCube.copy()\n",
    "    dsdCube_classes[classes_stacked != class_no] = np.nan\n",
    "    # calculate fsca evolution for the specific snow class\n",
    "    total_area = len(snow_classes[snow_classes == class_no])\n",
    "    fsca_class = np.nansum(dsdCube_classes,axis=(1,2))/total_area\n",
    "    fsca_class = fsca_class[idx_start:idx_end]\n",
    "    \n",
    "    fg,ax = plt.subplots()\n",
    "    # loop through the SWE coefficients of variation\n",
    "    for C in C_array:\n",
    "        # calculate the initial distribution based on the CoV and mean SWE estimate of the specific class\n",
    "        temp_dist = get_SWEdist(D,np.nanmean(SWE_map_base[snow_classes == class_no]),C)\n",
    "        # allow the initial distribution to vary by +- 10%, loop through those shifts\n",
    "        offsets = np.linspace(-0.1*np.nanmean(SWE_map_base),0.1*np.nanmean(SWE_map_base),20)\n",
    "        for offset in offsets:\n",
    "            # loop through the melt factors\n",
    "            for B in B_array:\n",
    "                # append into the subclass arrays\n",
    "                C_saver.append(C)\n",
    "                B_saver.append(B)\n",
    "                offset_saver.append(offset)\n",
    "                \n",
    "                # fSCA evolution for this combination of CV and melt factors\n",
    "                test_output = listonApproach(D,np.nanmean(SWE_map_base[snow_classes == class_no]),C,dailySW,daydiff_filt[0],B,offset) \n",
    "                ax.plot(test_output,color=[0.75,0.75,0.75])\n",
    "                # save the stats\n",
    "                MSE.append(np.mean((test_output[daydiff_filt]-fsca_class)**2))\n",
    "\n",
    "    # calculate the best-performing model parameters for this snow class\n",
    "    best_idx = np.where(MSE == np.nanmin(MSE))[0][0]\n",
    "    B_class.append(B_saver[best_idx])\n",
    "    C_class.append(C_saver[best_idx])\n",
    "    offset_class.append(offset_saver[best_idx])\n",
    "\n",
    "    # re-calculate the class-specific fSCA evolution from the best-performing parameters and plot\n",
    "    test_output = listonApproach(D,np.nanmean(SWE_map_base[snow_classes == class_no]),\n",
    "                                 C_class[-1],dailySW,daydiff_filt[0],B_class[-1],offset_class[-1]) \n",
    "    ax.plot(test_output,'--r')\n",
    "    # plot both the domain-scale (black) and class-specific (red) fSCA observed by Planet\n",
    "    ax.scatter(daydiff_filt,fsca_filt,20,'k',zorder=100)\n",
    "    ax.scatter(daydiff_filt,fsca_class,20,'r',zorder=200)\n",
    "\n",
    "### save the data, if desired ######\n",
    "# np.save(meadows_direc+'processed_'+str(year_value)+'/Method3_MeltFactor.npy',B_class)\n",
    "# np.save(meadows_direc+'processed_'+str(year_value)+'/Method3_CoV.npy',C_class)\n",
    "# np.save(meadows_direc+'processed_'+str(year_value)+'/Method3_offset.npy',offset_class)\n",
    "##################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d7a02-38de-47e9-a378-58c2f8b0d0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the initial estimated SWE distributions (top) and resulting fSCA evolution from Methods 2 and 3 (bottom)\n",
    "fg,ax = plt.subplots(2,1,figsize=(7.5,6.2))\n",
    "\n",
    "# specify colormap for the unique snow classes (see 4_process_SnowClass)\n",
    "cols = ['#000000','#9E5546','#E5782E','#F9C71F']\n",
    "\n",
    "# load parameters\n",
    "B_class = np.load(meadows_direc+'processed_'+str(year_value)+'/Method3_MeltFactor.npy')\n",
    "C_class = np.load(meadows_direc+'processed_'+str(year_value)+'/Method3_CoV.npy')\n",
    "offset_class = np.load(meadows_direc+'processed_'+str(year_value)+'/Method3_offset.npy')\n",
    "B_single = np.load(meadows_direc+'processed_'+str(year_value)+'/Method2_MeltFactor.npy')\n",
    "C_single = np.load(meadows_direc+'processed_'+str(year_value)+'/Method2_CoV.npy')\n",
    "offset_single = np.load(meadows_direc+'processed_'+str(year_value)+'/Method2_offset.npy')\n",
    "\n",
    "# initialize\n",
    "dist_holder = np.empty((class_numbers.shape[0],D.shape[0]))\n",
    "dist_holder[:] = np.nan\n",
    "melt_array = np.empty((class_numbers.shape[0],swsum.shape[0]))\n",
    "melt_array[:] = np.nan\n",
    "depth_array = np.empty((class_numbers.shape[0],D.shape[0]))\n",
    "depth_array[:] = np.nan\n",
    "\n",
    "# calculate the running cumulative shortwave radiation and melt array\n",
    "swsum = dailySW.copy()\n",
    "swsum[:daydiff_filt[0]] = 0\n",
    "swsum = np.cumsum(swsum)\n",
    "\n",
    "# loop through each class, determine the best SWE distribution and corresponding melt\n",
    "for classCount,classNo in enumerate(class_numbers):\n",
    "    class_meanSWE = np.mean(SWE_map_base[snow_classes == classNo])\n",
    "    temp = get_SWEdist(D,class_meanSWE,C_class[classCount])\n",
    "    temp = temp * (len(snow_classes[snow_classes == classNo])/len(snow_classes[~np.isnan(snow_classes)]))\n",
    "    dist_holder[classCount,:] = temp        \n",
    "    melt_array[classCount,:] = (swsum*B_class[classCount])     \n",
    "    depth_array[classCount,:] = D-offset_class[classCount]\n",
    "\n",
    "# interpolate the melt and resulting depth to a common benchmark (D)\n",
    "depth_array_corrected = depth_array.copy()\n",
    "dist_holder_corrected = dist_holder.copy()\n",
    "for classCount,classNo in enumerate(class_numbers):\n",
    "    depth_array_corrected[classCount,:] = D\n",
    "    fx = interp1d(depth_array[classCount,:],dist_holder[classCount,:],fill_value='extrapolate')\n",
    "    dist_holder_corrected[classCount,:] = fx(D)\n",
    "\n",
    "# plot the class-specific SWE distributions\n",
    "dist_holder_corrected = dist_holder_corrected/np.sum(dist_holder_corrected)\n",
    "for classCount,classNo in enumerate(class_numbers):\n",
    "    ax[0].plot(D,dist_holder_corrected[classCount,:],\n",
    "               color=cols[classCount],linewidth=2,label='_nolegend_')\n",
    "# inputting for the legend formatting        \n",
    "ax[0].plot([np.nan,np.nan],[np.nan,np.nan],color='k',linewidth=2,label='Method 3')\n",
    "\n",
    "# calculate the SWE distribution and resulting fSCA for the domain-scale method (Method 2)\n",
    "SWE_dist_single = get_SWEdist(D,np.nanmean(SWE_map_base),C_single)\n",
    "fx = interp1d(D-offset_single,SWE_dist_single,fill_value='extrapolate')\n",
    "SWE_dist_single = fx(D)\n",
    "ax[0].plot(D,SWE_dist_single,'--k',label='Method 2')\n",
    "fsca_variable = []\n",
    "for dtt in np.arange(melt_array.shape[1]):\n",
    "    # print(dtt)\n",
    "    temp = depth_array_corrected - np.tile(melt_array[:,dtt],[D.shape[0],1]).T\n",
    "    fsca_variable.append(np.sum(dist_holder_corrected[temp > 0])) \n",
    "fsca_variable = np.array(fsca_variable)\n",
    "test = listonApproach(D,np.nanmean(SWE_map_base),C_single,dailySW,daydiff_filt[0],B_single,offset_single) \n",
    "timeline = np.arange(len(test)).astype(float)\n",
    "fsca_variable[timeline < daydiff_filt[0]] = np.nan\n",
    "fsca_variable[timeline > np.max(daydiff_filt)] = np.nan\n",
    "test[timeline < daydiff_filt[0]] = np.nan\n",
    "test[timeline > np.max(daydiff_filt)] = np.nan\n",
    "# specify the dates to plot agains\n",
    "date_list = [datetime.datetime(year_value-1, 10, 1) + datetime.timedelta(days=d) for d in timeline]\n",
    "\n",
    "# plot the domain-scale fSCA evolution for both Method 2 and Method 3\n",
    "ax[1].plot(date_list,fsca_variable,'-k',label='Method 3')\n",
    "ax[1].plot(date_list,test,'--k',label='Method 2')\n",
    "\n",
    "# plot the PlanetScope fSCA observations for comparison\n",
    "obs_date_list = [datetime.datetime(year_value-1, 10, 1) + datetime.timedelta(days=d) for d in daydiff_filt.astype(list)]\n",
    "ax[1].scatter(obs_date_list,fsca_filt,20,'k',zorder=100,label='Planet observations')\n",
    "\n",
    "# plot formatting\n",
    "ax[0].set_xlim([0,2.5])\n",
    "ax[0].set_xlabel('Snow Water Equivalent [m]')\n",
    "ax[0].set_ylabel('PDF')\n",
    "ax[0].legend()\n",
    "ax[0].set_title('Station ID: '+DOMID+', Water Year: '+str(year_value))\n",
    "ax[1].set_ylabel('fractional\\nsnow-covered area')\n",
    "ax[1].legend()\n",
    "date_formatter = DateFormatter('%b %d')\n",
    "ax[1].xaxis.set_major_formatter(date_formatter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo",
   "language": "python",
   "name": "pangeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
