{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d132c-32c4-4a57-ba7d-f5fa9e63e222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies and plot formatting options\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio as rio\n",
    "import datetime\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import statistics\n",
    "\n",
    "plt.rcParams['font.family'] = 'DeJavu Serif'\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6998b4-2482-4e31-8ee8-4e43c4165e54",
   "metadata": {},
   "source": [
    "## SET USER DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e2bdab-b923-4777-8276-944ed29c58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the years to process over\n",
    "years = [2019,2020,2021,2022,2023]\n",
    "# specify the focus year for saving purposes\n",
    "year_select = 2022\n",
    "# specify the index of the first PS image to use for each of the \"years\" above\n",
    "# 0 means that DSD will be calculated using all snow cover maps\n",
    "starting_array = [21,30,27,81,65]\n",
    "\n",
    "# domain ID and file directories\n",
    "DOMID = 'DPO'\n",
    "data_direc = '/Users/jpflug/Documents/Projects/cubesatReanaly/Data/Meadows/'+DOMID+'/self_classified/SCA/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7d23eb-8676-4005-ae49-1e750738a4b5",
   "metadata": {},
   "source": [
    "## FUNCTIONS -- DO NOT EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4805d8f-e5fe-4a91-8cea-f0fcab6d89aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the days from 1 October for each image and create the snow cover data cube (time,y,x)\n",
    "def DSD_index(year,files):\n",
    "    # instantiate\n",
    "    data_list,daydiff = [],[]\n",
    "    # set the reference date to the start of the water year\n",
    "    ref_date = datetime.date(year-1,10,1)\n",
    "\n",
    "    # loop through the snow cover observations\n",
    "    for fCount,file in enumerate(files):\n",
    "        # determine/log date\n",
    "        breakFile = file.split('/')[-1].split('_')[0]\n",
    "        dater = datetime.date(int(breakFile[0:4]),int(breakFile[4:6]),int(breakFile[6:8]))\n",
    "\n",
    "        # determine how many days into the water year the PS obs is from\n",
    "        daydiff.append((dater-ref_date).days)\n",
    "\n",
    "        # read in and append the data\n",
    "        with rio.open(file) as src:\n",
    "            data = src.read(1)\n",
    "            data_list.append(data)\n",
    "\n",
    "    # create stacked dataframe for snow cover\n",
    "    stacked_data = np.stack(data_list, axis=0).astype(float)\n",
    "    daydiff = np.array(daydiff)\n",
    "    # print(daydiff)\n",
    "    return stacked_data,daydiff\n",
    "\n",
    "# function for correcting no-snow dates preceded and followed by snow cover (SEASONAL SNOW ONLY!!)\n",
    "def find_spurious_0(arr):\n",
    "    locations = []\n",
    "    time, x, y = arr.shape\n",
    "    for t in range(1, time - 1):\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                if arr[t-1, i, j] == 1 and arr[t, i, j] == 0 and arr[t+1, i, j] == 1:\n",
    "                    locations.append((t, i, j))\n",
    "    return locations\n",
    "# function for correcting snow dates preceded and followed by no snow cover (SEASONAL SNOW ONLY!!)\n",
    "def find_spurious_1(arr):\n",
    "    locations = []\n",
    "    time, x, y = arr.shape\n",
    "    for t in range(1, time - 1):\n",
    "        for i in range(x):\n",
    "            for j in range(y):\n",
    "                if arr[t-1, i, j] == 0 and arr[t, i, j] == 1 and arr[t+1, i, j] == 0:\n",
    "                    locations.append((t, i, j))\n",
    "    return locations\n",
    "# find the last dates with consecute snow-present observations\n",
    "def find_last_consecutive_ones_after_zero(arr):\n",
    "    max_consecutive_indices = []\n",
    "    n = len(arr)\n",
    "    i = 0\n",
    "    while i < n - 1:\n",
    "        if arr[i] == 0 and arr[i + 1] == 1:\n",
    "            start_index = i + 1\n",
    "            while start_index < n - 1 and arr[start_index] == 1 and arr[start_index + 1] == 1:\n",
    "                start_index += 1\n",
    "            consecutive_indices = [start_index]\n",
    "            if len(consecutive_indices) > len(max_consecutive_indices):\n",
    "                max_consecutive_indices = consecutive_indices\n",
    "            i = start_index + 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return max_consecutive_indices\n",
    "# fill spurious values using the defined \"fill_value\"\n",
    "def fill_locations(arr, locations,fill_value):\n",
    "    for loc in locations:\n",
    "        t, x, y = loc\n",
    "        arr[t, x, y] = fill_value\n",
    "    return arr\n",
    "def fill_nodata(stacked_data):\n",
    "    t,y,x = stacked_data.shape\n",
    "    for tt in np.arange(t):\n",
    "        tt_date = stacked_data[tt,:,:]\n",
    "        if tt == 0:\n",
    "            tt_date[tt_date == 2] = 1\n",
    "        else:\n",
    "            tt_ref = stacked_data[tt-1,:,:]\n",
    "            tt_date[tt_date == 2] = tt_ref[tt_date == 2] \n",
    "        stacked_data[tt,:,:] = tt_date\n",
    "    return stacked_data\n",
    "# perform data QA and QC using the functions from above (SEASONAL SNOW ONLY!!)\n",
    "def DSD_qaqc(stacked_data):\n",
    "    locations_1 = find_spurious_0(stacked_data)\n",
    "    stacked_data = fill_locations(stacked_data,locations_1,1)\n",
    "    locations_2 = find_spurious_1(stacked_data)\n",
    "    stacked_data = fill_locations(stacked_data,locations_2,0)\n",
    "    _,y,x = stacked_data.shape\n",
    "    for j in range(y):\n",
    "        for i in range(x):\n",
    "            location_3 = find_last_consecutive_ones_after_zero(stacked_data[:,j,i])\n",
    "            if len(location_3) > 0:\n",
    "                stacked_data[:location_3[0],j,i] = 1\n",
    "    return stacked_data\n",
    "\n",
    "def fill_DSDdates(daydiff,dsd,nodata_ref):\n",
    "    # determine the actual day since October 1 the observation came from\n",
    "    dsd_date = daydiff[dsd].astype(float)\n",
    "    \n",
    "    temp = np.empty(dsd.shape)\n",
    "    # loop through the dsd array\n",
    "    # determine the snow classification on the dsd date\n",
    "    for j in range(dsd.shape[0]):\n",
    "        for i in range(dsd.shape[1]):\n",
    "            time_index = dsd[j,i]\n",
    "            temp[j,i] = nodata_ref[time_index,j,i]\n",
    "            \n",
    "    # determine where when dsd occurred on a bad observation pixel account for that in the uncertainty\n",
    "    filled_y,filled_x = np.where(temp == 2)\n",
    "    if len(filled_y) > 0:\n",
    "        for j,i in zip(filled_y,filled_x):\n",
    "            time_index = dsd[j,i]\n",
    "            temp = nodata_ref[time_index,j,i]\n",
    "            while temp == 2:\n",
    "                time_index = dsd[j,i] - 1\n",
    "                temp = nodata_ref[time_index,j,i]\n",
    "            # fill the dsd with the last good date\n",
    "            dsd[j,i] = time_index\n",
    "    \n",
    "    # determine the snow-present day just before that\n",
    "    temp = daydiff[dsd-1].astype(float)\n",
    "    # and calculate the uncertainty\n",
    "    temp[dsd < 0] = np.nan\n",
    "    dsd_uncertain = dsd_date-temp\n",
    "    # set the dsd as the halfway point\n",
    "    dsd_date = np.ceil((dsd_date+temp)/2)#.astype(int)\n",
    "    return dsd_date,dsd_uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f047430f-aecd-4988-8ed3-a51e8546f534",
   "metadata": {},
   "source": [
    "#### CALCULATE THE DSD AND DSD UNCERTAINTY FOR EACH PIXEL AND YEAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6da3a4c-f75d-416d-b43d-bc582740b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through each testing year\n",
    "for year_index,testing_year in enumerate(years):\n",
    "    # load the processed snow cover files\n",
    "    print('year: ',year_index)\n",
    "    files = sorted(glob.glob(data_direc+str(testing_year)+'*SCA.tif'))[starting_array[year_index]:]\n",
    "\n",
    "    # initialize\n",
    "    ref_date = datetime.date(testing_year-1,10,1)\n",
    "    data_list,daydiff = [],[]\n",
    "    # loop through the good planet scenes\n",
    "    for fCount,file in enumerate(files):\n",
    "        # determine the date from the filename and append to the record\n",
    "        breakFile = file.split('/')[-1].split('_')[0]\n",
    "        dater = datetime.date(int(breakFile[0:4]),int(breakFile[4:6]),int(breakFile[6:8]))\n",
    "        daydiff.append((dater-ref_date).days)\n",
    "\n",
    "        # read in and store the data\n",
    "        with rio.open(file) as src:\n",
    "            data = src.read(1)\n",
    "            data_list.append(data)\n",
    "\n",
    "    # create a stacked data array for snow cover\n",
    "    stacked_data = np.stack(data_list, axis=0).astype(float)\n",
    "    daydiff = np.array(daydiff)\n",
    "\n",
    "    # fill instances with no-data classifications (from image artifacts)\n",
    "    nodata_ref = stacked_data.copy()\n",
    "    stacked_data = fill_nodata(stacked_data)\n",
    "\n",
    "    # do data filtering and post-processing (SEASONAL SNOW ONLY!!)\n",
    "    stacked_data = DSD_qaqc(stacked_data)\n",
    "\n",
    "    # save the data if this is a modeling year\n",
    "    if testing_year == year_select:\n",
    "        np.save(data_direc+'processed_'+str(year_select)+'/DSD_stacked.npy',stacked_data)\n",
    "        np.save(data_direc+'processed_'+str(year_select)+'/DSD_daydiff.npy',daydiff)\n",
    "\n",
    "    # calculate the date of snow disappearance\n",
    "    dsd = np.argmin(stacked_data,axis=0)\n",
    "\n",
    "    # determine the actual dsd date and uncertainty\n",
    "    dsd_masked = dsd.copy()\n",
    "    dsd,dsd_uncertain = fill_DSDdates(daydiff,dsd,nodata_ref)\n",
    "    # mask values where snow was never observed\n",
    "    dsd[dsd_masked == 0] = np.nan\n",
    "\n",
    "    # calculate the domain annual-normalized pattern\n",
    "    dsd_anomaly = dsd-np.nanmedian(dsd)\n",
    "\n",
    "    # append the data \n",
    "    if year_index == 0:\n",
    "        shpp = dsd_anomaly.shape\n",
    "        dsd_saver = np.empty((len(years),shpp[0],shpp[1]))\n",
    "        dsd_anomaly_saver = np.empty((len(years),shpp[0],shpp[1]))\n",
    "        dsd_uncertain_saver = np.empty((len(years),shpp[0],shpp[1]))    \n",
    "    dsd_saver[year_index,:,:] = dsd\n",
    "    dsd_anomaly_saver[year_index,:,:] = dsd_anomaly\n",
    "    dsd_uncertain_saver[year_index,:,:] = dsd_uncertain\n",
    "\n",
    "######## save additional data, as desired ##############\n",
    "# np.save(data_direc+'processed_'+str(year_select)+'/DSD.npy',dsd_saver)\n",
    "# np.save(data_direc+str(year_select)+'/DSD_anomaly.npy',dsd_anomaly_saver)\n",
    "# np.save(data_direc+'/DSD_uncertain_saver.npy',dsd_uncertain_saver)\n",
    "\n",
    "# holder = xr.open_dataset(files[0])\n",
    "# holder['band_data'][0,:,:] = np.mean(dsd_anomaly_saver,axis=0)\n",
    "# holder['band_data'].rio.to_raster(data_direc+'averageDSD_anomaly.tif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04c2075-4a8d-4f1b-b7f4-a6036be3de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform plotting for the DSD and DSD uncertainty\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "mask = np.sum(dsd_saver,axis=0)\n",
    "\n",
    "fg,ax = plt.subplots(2,len(years),figsize=(9,5),\n",
    "                     sharex='col', sharey='row', gridspec_kw={'height_ratios': [1, 1],'hspace': 0.02, 'wspace': 0.05})\n",
    "\n",
    "for idx,year in enumerate(years):\n",
    "    data = dsd_anomaly_saver[idx,:,:]\n",
    "    data[np.isnan(mask)] = np.nan\n",
    "    try:\n",
    "        out1 = ax[1,idx].imshow(data,vmin=-20,vmax=20,interpolation='none',cmap='RdBu')\n",
    "        ax[0,idx].set_title(year)\n",
    "    except:\n",
    "        out1 = ax[1].imshow(data,vmin=-20,vmax=20,interpolation='none',cmap='RdBu')\n",
    "        ax[0].set_title(year)\n",
    "    \n",
    "for idx,year in enumerate(years):\n",
    "    # data = dsd_uncertain_saver[idx,:,:]\n",
    "    data = dsd_saver[idx,:,:]\n",
    "    data[np.isnan(mask)] = np.nan\n",
    "    try:\n",
    "        out2 = ax[0,idx].imshow(data,vmin=180,vmax=280,interpolation='none',cmap='inferno')\n",
    "    except:\n",
    "        out2 = ax[0].imshow(data,vmin=0,vmax=14,interpolation='none',cmap='Greens')\n",
    "    \n",
    "for axx in np.ravel(ax):\n",
    "    axx.set_xticks([])\n",
    "    axx.set_yticks([])\n",
    "    axx.set_facecolor([0.6,0.6,0.6])\n",
    "    \n",
    "pos_bott = ax[1,-1].get_position()\n",
    "pos_top = ax[0,-1].get_position()\n",
    "\n",
    "cax1 = fg.add_axes([pos_bott.x1 + 0.02,pos_bott.y0,0.02,pos_top.y1-pos_bott.y0])\n",
    "cbar1 = fg.colorbar(out2, cax=cax1, orientation='vertical')\n",
    "cbar1.set_label('Date of snow disappearance [days from 1 Oct]',fontsize=11)\n",
    "  \n",
    "cax2 = fg.add_axes([pos_bott.x1 + 0.12,pos_bott.y0,0.02,pos_top.y1-pos_bott.y0])\n",
    "cbar2 = fg.colorbar(out1, cax=cax2, orientation='vertical')\n",
    "cbar2.set_label('Date of snow disappearance\\nanomaly [days]',fontsize=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pangeo",
   "language": "python",
   "name": "pangeo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
